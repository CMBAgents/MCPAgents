{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b04378-221f-45ce-a9cc-cb58467b919d",
   "metadata": {},
   "source": [
    "Authors: Licong Xu and Boris Bolliet (Cambridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ed7aaa-e6c9-42c4-bad6-b5c16176ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.sse import sse_client\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "from autogen import LLMConfig\n",
    "from autogen.agentchat import AssistantAgent\n",
    "from autogen.mcp import create_toolkit\n",
    "import json\n",
    "import anyio\n",
    "import asyncio\n",
    "\n",
    "# Only needed for Jupyter notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from autogen.agentchat.group import (\n",
    "    AgentNameTarget,\n",
    "    AgentTarget,\n",
    "    AskUserTarget,\n",
    "    ContextExpression,\n",
    "    ContextStr,\n",
    "    ContextStrLLMCondition,\n",
    "    ContextVariables,\n",
    "    ExpressionAvailableCondition,\n",
    "    ExpressionContextCondition,\n",
    "    GroupChatConfig,\n",
    "    GroupChatTarget,\n",
    "    Handoffs,\n",
    "    NestedChatTarget,\n",
    "    OnCondition,\n",
    "    OnContextCondition,\n",
    "    ReplyResult,\n",
    "    RevertToUserTarget,\n",
    "    SpeakerSelectionResult,\n",
    "    StayTarget,\n",
    "    StringAvailableCondition,\n",
    "    StringContextCondition,\n",
    "    StringLLMCondition,\n",
    "    TerminateTarget,\n",
    ")\n",
    "\n",
    "from autogen.agentchat.group.patterns import (\n",
    "    DefaultPattern,\n",
    "    ManualPattern,\n",
    "    AutoPattern,\n",
    "    RandomPattern,\n",
    "    RoundRobinPattern,\n",
    ")\n",
    "\n",
    "\n",
    "from autogen import ConversableAgent, UpdateSystemMessage\n",
    "from autogen.agents.experimental import DocAgent\n",
    "import os\n",
    "import copy\n",
    "from typing import Any, Dict, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from autogen.agentchat import initiate_group_chat, a_initiate_group_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56fa18f6-1438-482a-83d6-ef2a04254e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the arxiv MCP server\n",
    "mcp_server_path = Path(\"mcp_arxiv.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14fded9e-0818-413c-8f54-59b16adea138",
   "metadata": {},
   "outputs": [],
   "source": [
    "joker_message = \"\"\"\n",
    "You are the joker in the team. You make jokes in the style of shakespeare. \n",
    "\n",
    "You must obey the following constraints:\n",
    "\n",
    "{joke_constraints}\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class JokeResponse(BaseModel):\n",
    "    joke_instructions: str = Field(..., description=\"instruction, not in the style of Shakespeare\")     \n",
    "    joke: str = Field(..., description=\"joke in the style of Shakespeare\")\n",
    "    joke_explanation: str = Field(..., description=\"explanation, not in the style of Shakespeare\")\n",
    "    def format(self) -> str:\n",
    "        return \"\\n\".join([\n",
    "            \"**Joke instructions:**\",\n",
    "            \"\",\n",
    "            self.joke_instructions,\n",
    "            \"\",\n",
    "            \"**Joke:**\",\n",
    "            \"\",\n",
    "            self.joke,\n",
    "            \"\",\n",
    "            \"**Joke explanation:**\",\n",
    "            \"\",\n",
    "            self.joke_explanation\n",
    "        ])\n",
    "\n",
    "\n",
    "default_llm_config = {'cache_seed': 42,\n",
    "                     'temperature': 1.,\n",
    "                     'top_p': 0.05,\n",
    "                     'config_list': [{'model': 'gpt-4o',\n",
    "                                      'api_key': os.getenv('OPENAI_API_KEY'),\n",
    "                                      'api_type': 'openai'}],\n",
    "                     'timeout': 1200}\n",
    "\n",
    "joker_config_list = copy.deepcopy(default_llm_config)\n",
    "joker_config_list['config_list'][0]['response_format'] = JokeResponse\n",
    "\n",
    "\n",
    "joker =  ConversableAgent(\n",
    "    name=\"joker\",\n",
    "    system_message=joker_message,\n",
    "    # llm_config=LLMConfig(model=\"gpt-4o\", \n",
    "    #                      api_type=\"openai\",\n",
    "    #                      response_format=JokeResponse\n",
    "    #                     ),\n",
    "    llm_config = joker_config_list,\n",
    "    update_agent_state_before_reply=[UpdateSystemMessage(joker_message),],\n",
    ")\n",
    "\n",
    "workflow_context = ContextVariables(data={\n",
    "    \"joke_constraints\": \"the joke should make use of the contextual information passed on to you. It should be a paragraph long and use as much detailed information from the context as possible.\",\n",
    "})\n",
    "\n",
    "\n",
    "task = \"\"\"\n",
    "Make a joke based on the title and abstract of an arxiv paper of your choice.\n",
    "\"\"\"\n",
    "\n",
    "initial_agent = joker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5935d8d-e63d-4cdd-b0fc-869064d5e7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all agents reset\n",
      ".cache folder deleted.\n",
      "\u001b[33m_User\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "Make a joke based on the title and abstract of an arxiv paper of your choice.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mmcp_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_sviIr1EV6z6JE8YRJI0yfET3): search_arxiv *****\u001b[0m\n",
      "Arguments: \n",
      "{\"query\":\"quantum computing\",\"max_results\":1}\n",
      "\u001b[32m*****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION search_arxiv...\n",
      "Call ID: call_sviIr1EV6z6JE8YRJI0yfET3\n",
      "Input arguments: {'query': 'quantum computing', 'max_results': 1}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_sviIr1EV6z6JE8YRJI0yfET3) *****\u001b[0m\n",
      "('2208.00733v1', None)\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mmcp_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_1Rc3gVt9tIaVdnDh4PiaM4zz): download_paper *****\u001b[0m\n",
      "Arguments: \n",
      "{\"arxiv_id\": \"2208.00733v1\"}\n",
      "\u001b[32m*******************************************************************************\u001b[0m\n",
      "\u001b[32m***** Suggested tool call (call_baJer7yUAgP7DPFq2vxWaOdo): get_paper_info *****\u001b[0m\n",
      "Arguments: \n",
      "{\"arxiv_id\": \"2208.00733v1\"}\n",
      "\u001b[32m*******************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION download_paper...\n",
      "Call ID: call_1Rc3gVt9tIaVdnDh4PiaM4zz\n",
      "Input arguments: {'arxiv_id': '2208.00733v1'}\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION get_paper_info...\n",
      "Call ID: call_baJer7yUAgP7DPFq2vxWaOdo\n",
      "Input arguments: {'arxiv_id': '2208.00733v1'}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_1Rc3gVt9tIaVdnDh4PiaM4zz) *****\u001b[0m\n",
      "('Downloaded 2208.00733v1 to 2208.00733v1.pdf', None)\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m***** Response from calling tool (call_baJer7yUAgP7DPFq2vxWaOdo) *****\u001b[0m\n",
      "('{\"title\": \"The Rise of Quantum Internet Computing\", \"abstract\": \"This article highlights quantum Internet computing as referring to\\\\ndistributed quantum computing over the quantum Internet, analogous to\\\\n(classical) Internet computing involving (classical) distributed computing over\\\\nthe (classical) Internet. Relevant to quantum Internet computing would be areas\\\\nof study such as quantum protocols for distributed nodes using quantum\\\\ninformation for computations, quantum cloud computing, delegated verifiable\\\\nblind or private computing, non-local gates, and distributed quantum\\\\napplications, over Internet-scale distances.\"}', None)\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mmcp_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_7r2dhL7uEuZTNZXZd1EKr6Bg): transfer_to_joker_1 *****\u001b[0m\n",
      "Arguments: \n",
      "{}\n",
      "\u001b[32m************************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION transfer_to_joker_1...\n",
      "Call ID: call_7r2dhL7uEuZTNZXZd1EKr6Bg\n",
      "Input arguments: {}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_7r2dhL7uEuZTNZXZd1EKr6Bg) *****\u001b[0m\n",
      "Transfer to joker\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mjoker\u001b[0m (to chat_manager):\n",
      "\n",
      "**Joke instructions:**\n",
      "\n",
      "Make a joke about quantum internet computing.\n",
      "\n",
      "**Joke:**\n",
      "\n",
      "Hark! What light through yonder quantum network breaks? 'Tis the rise of the quantum Internet, and Juliet is the sun, shining upon distributed nodes with quantum grace. But soft, what protocols doth this new dawn bring? Forsooth, 'tis a tale of entangled hearts and non-local gates, where computations dance like sprites upon the ether. Verily, if Romeo were to send a message to his fair Juliet, 'twould be through quantum cloud computing, with nary a delay, for even the speed of light doth pale in comparison to the swiftness of love's quantum leap!\n",
      "\n",
      "**Joke explanation:**\n",
      "\n",
      "The joke plays on the famous balcony scene from Shakespeare's 'Romeo and Juliet', using the context of quantum internet computing. It humorously imagines a world where Romeo and Juliet communicate through advanced quantum protocols, highlighting the futuristic and almost magical nature of quantum computing. The joke also references concepts like distributed nodes and non-local gates, which are part of the paper's abstract, to create a whimsical scenario where love transcends even the speed of light.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (40075c0b-69c4-4b9b-a298-d277dd221a70): No next speaker selected\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "async def create_toolkit_and_run(session: ClientSession) -> None:\n",
    "    # Create a toolkit with available MCP tools\n",
    "    toolkit = await create_toolkit(session=session)\n",
    "    mcp_agent = ConversableAgent(name=\"mcp_agent\", \n",
    "                             system_message=r\"\"\"\n",
    "Download arxiv paper and extract titles and abstracts. \n",
    "                             \"\"\",\n",
    "                             llm_config=LLMConfig(model=\"gpt-4o\", \n",
    "                                                  api_type=\"openai\",\n",
    "                                                  tool_choice=\"required\"\n",
    "                                                 ))\n",
    "    # Register MCP tools with the agent\n",
    "    toolkit.register_for_llm(mcp_agent)\n",
    "    \n",
    "    toolkit.register_for_execution(mcp_agent)\n",
    "\n",
    "    # joker.handoffs.set_after_work(AgentTarget(mcp_agent))\n",
    "    joker.handoffs.set_after_work(TerminateTarget())\n",
    "    \n",
    "    mcp_agent.handoffs.set_after_work(AgentTarget(joker))\n",
    "\n",
    "\n",
    "    mcp_agent.handoffs.add_llm_conditions([\n",
    "            OnCondition(\n",
    "                target=AgentTarget(joker),\n",
    "                condition=StringLLMCondition(prompt=\"The papers have been downloaded.\"),\n",
    "                # available=StringAvailableCondition(context_variable=\"requires_login\"),\n",
    "            ),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    agents=[joker,\n",
    "            mcp_agent,\n",
    "               ]\n",
    "    \n",
    "    for agent in agents:\n",
    "        agent.reset()\n",
    "    print(\"all agents reset\")\n",
    "\n",
    "    import shutil\n",
    "    import os\n",
    "    \n",
    "    def delete_cache_folder():\n",
    "        cache_path = os.path.join(os.getcwd(), \".cache\")\n",
    "        if os.path.isdir(cache_path):\n",
    "            shutil.rmtree(cache_path)\n",
    "            print(\".cache folder deleted.\")\n",
    "        else:\n",
    "            print(\"No .cache folder found in current directory.\")\n",
    "    \n",
    "    delete_cache_folder()\n",
    "\n",
    "    # Create the pattern\n",
    "    agent_pattern = DefaultPattern(\n",
    "      agents=[joker, mcp_agent],\n",
    "      initial_agent=mcp_agent,\n",
    "      context_variables=workflow_context,\n",
    "    )\n",
    "    \n",
    "\n",
    "    await a_initiate_group_chat(\n",
    "            pattern=agent_pattern,\n",
    "            messages=task,\n",
    "            max_rounds=20,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[str(mcp_server_path), \"stdio\", \"--storage-path\", \"arxiv_papers\"]\n",
    ")\n",
    "\n",
    "async with stdio_client(server_params) as (read, write), ClientSession(read, write) as session:\n",
    "    # Initialize the connection\n",
    "    await session.initialize()\n",
    "    await create_toolkit_and_run(session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp_env",
   "language": "python",
   "name": "mcp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
